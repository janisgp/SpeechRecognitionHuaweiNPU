{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data_loader import data_loader\n",
    "from model import get_cnn_lstm_t_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'prep-train-clean-copy-shuffled/train/0.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-82c897ca8193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mNt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m247000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mNv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m59500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'0.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'0_len.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prep-train-clean-copy-shuffled/train/0.npy'"
     ]
    }
   ],
   "source": [
    "train_folder = 'prep-train-clean/train/'\n",
    "val_folder = 'prep-train-clean/val/'\n",
    "train_folder = 'prep-train-clean-copy-shuffled/train/'\n",
    "val_folder = 'prep-train-clean-copy-shuffled/val/'\n",
    "\n",
    "Nt = 247000\n",
    "Nv = 59500\n",
    "x = np.load(train_folder + '0.npy')\n",
    "input_length = np.load(train_folder + '0_len.npy')\n",
    "\n",
    "y = np.load(train_folder + '0_target.npy')\n",
    "label_length = np.load(train_folder + '0_target_len.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "config = {\n",
    "    'lr': 0.001,\n",
    "    'patience': 6,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'hidden_dim': 384,\n",
    "    't_step': 480,\n",
    "    'signal_dim': 160,\n",
    "    'lstm_layers': 1,\n",
    "    'print_every': 100,\n",
    "    'num_classes_internal': 32,\n",
    "    'num_classes': 29,\n",
    "    'chunk_size': 1024,\n",
    "    'f_size': 32\n",
    "}\n",
    "\n",
    "config['steps_per_epoch_train'] = int(Nt/config['batch_size'])\n",
    "config['steps_per_epoch_val'] = int(Nv/config['f_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9360f1a6765c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cnn_lstm_t_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = get_cnn_lstm_t_dense(config)\n",
    "model.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1551008\n"
     ]
    }
   ],
   "source": [
    "# Get number of trainable paramters for comparison with keras\n",
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### not needed for training\n",
    "# save model for huawei compatibility check\n",
    "#####################################\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess,\n",
    "    tf.get_default_graph().as_graph_def(),\n",
    "    ['out']\n",
    "    )\n",
    "\n",
    "# save graph\n",
    "with tf.gfile.GFile('best_model.pb', \"wb\") as f:\n",
    "    f.write(output_graph_def.SerializeToString())\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### not needed for training\n",
    "#######################\n",
    "# sanity check \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# test forward pass\n",
    "# result = sess.run(model.save_nodes[0], {\n",
    "#     model.inputs[0]: np.ones((config['batch_size'], config['t_step']*config['signal_dim'])),\n",
    "#     model.inputs[1]: np.zeros((config['batch_size'], config['hidden_dim'])),\n",
    "#     model.inputs[2]: np.ones(config['batch_size'])*config['t_step']\n",
    "# })\n",
    "result = sess.run(model.outputs[0], {\n",
    "    model.inputs[0]: np.ones((config['batch_size'], config['t_step']*config['signal_dim'])),\n",
    "    model.inputs[1]: np.zeros((config['batch_size'], config['hidden_dim'])),\n",
    "    model.inputs[2]: np.ones(config['batch_size'])*config['t_step']\n",
    "})\n",
    "\n",
    "result.shape\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor 'Placeholder:0' shape=(?, 76800) dtype=float32>,\n",
       "  <tf.Tensor 'Placeholder_1:0' shape=(?, 384) dtype=float32>],\n",
       " [<tf.Tensor 'CheckNumerics:0' shape=(?, 480, 29) dtype=float32>],\n",
       " [<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7faec169e7b8>],\n",
       " [<tf.Tensor 'out:0' shape=(?, 15360) dtype=float32>])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs, model.outputs, model.targets, model.save_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = data_loader(train_folder, config, mode='train')\n",
    "data_loader_val = data_loader(val_folder, config, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3859 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train on 8523 samples\n"
     ]
    }
   ],
   "source": [
    "############# Training ##########\n",
    "#################################\n",
    "\n",
    "import datetime\n",
    "\n",
    "the_labels = tf.placeholder(shape=(None, None), dtype=tf.float32)\n",
    "input_lengths = tf.placeholder(shape=(None,), dtype=tf.int64)\n",
    "label_lengths = tf.placeholder(shape=(None,), dtype=tf.int64)\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "ctc_cost = ctc_lambda_func(\n",
    "    [model.outputs[0], the_labels, input_lengths, label_lengths]\n",
    ")\n",
    "ctc_cost = tf.reduce_mean(ctc_cost)\n",
    "\n",
    "train_ops = tf.train.AdamOptimizer(learning_rate).minimize(ctc_cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "overall_train_loss, overall_val_loss = [], []\n",
    "run_id = str(datetime.datetime.now())\n",
    "best_val_loss = np.inf\n",
    "orig_lr = config['lr']\n",
    "for i in range(config['epochs']):\n",
    "    print('Epoch ' + str(i+1))\n",
    "\n",
    "    # Training\n",
    "    train_loss = []\n",
    "    for j in tqdm(range(config['steps_per_epoch_train'])):\n",
    "\n",
    "        batch_x, batch_x_len, batch_y, batch_y_len = data_loader_train.__next__()\n",
    "\n",
    "        inp = { \n",
    "            model.inputs[0]: batch_x,\n",
    "            model.inputs[1]: np.zeros((batch_x.shape[0], config['hidden_dim'])),\n",
    "            the_labels: batch_y,\n",
    "            input_lengths: batch_x_len,\n",
    "            label_lengths: batch_y_len,\n",
    "            learning_rate: config['lr']\n",
    "        }\n",
    "\n",
    "        cost = sess.run([ctc_cost, train_ops], inp)\n",
    "        train_loss.append(cost[0])\n",
    "        if j % config['print_every'] == 0:\n",
    "#             print(cost[0])\n",
    "            pass\n",
    "#             logs, outs = sess.run([model.outputs[0], model.save_nodes[0]], inp)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = []\n",
    "    for j in tqdm(range(config['steps_per_epoch_val'])):\n",
    "#         }\n",
    "\n",
    "        batch_x, batch_x_len, batch_y, batch_y_len = data_loader_val.__next__()\n",
    "    \n",
    "        inp = { \n",
    "            model.inputs[0]: batch_x,\n",
    "            model.inputs[1]: np.zeros((batch_x.shape[0], config['hidden_dim'])),\n",
    "            the_labels: batch_y,\n",
    "            input_lengths: batch_x_len,\n",
    "            label_lengths: batch_y_len\n",
    "        }\n",
    "        \n",
    "        cost = sess.run([ctc_cost], inp)\n",
    "        \n",
    "        val_loss.append(cost[0])\n",
    "\n",
    "    t_loss = np.mean(train_loss)\n",
    "    v_loss = np.mean(val_loss)\n",
    "    print('Epoch ' + str(i+1) + ' | Training loss: ' + str(t_loss) + ' | Validation loss: ' + str(v_loss))\n",
    "    overall_train_loss.append(t_loss)\n",
    "    overall_val_loss.append(v_loss)\n",
    "\n",
    "    if v_loss < best_val_loss:\n",
    "\n",
    "        best_val_loss = v_loss\n",
    "        # save graph\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            ['out']\n",
    "            )\n",
    "\n",
    "        # save graph\n",
    "        with tf.gfile.GFile('best_model ' + run_id + '.pb', \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "    else:\n",
    "        config['lr'] = 0.5*config['lr']\n",
    "        print('new lr: ' + str(config['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x = x_val[0*config['batch_size']:(0+1)*config['batch_size']]\n",
    "batch_x = np.reshape(batch_x, (config['batch_size'], config['t_step']*config['signal_dim']))\n",
    "\n",
    "y_text = y_val[0*config['batch_size']:(0+1)*config['batch_size']]\n",
    "batch_y = sparse_tuple_from(y_text)\n",
    "\n",
    "inp = { \n",
    "    model.inputs[0]: batch_x,\n",
    "    model.inputs[1]: np.zeros((config['batch_size'], config['hidden_dim'])),\n",
    "    model.inputs[2]: np.ones(config['batch_size'])*config['t_step'],\n",
    "    model.targets[0]: batch_y\n",
    "}\n",
    "\n",
    "cost = sess.run(model.outputs[0], inp)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in tf.get_default_graph().as_graph_def().node:\n",
    "    print(n.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Load best graph\n",
    "sess = tf.Session()\n",
    "# insert ur model ...\n",
    "model_filename = 'runs/2019-01-14 00:26:20.816162/best_model.pb'\n",
    "with gfile.FastGFile(model_filename, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    g_in = tf.import_graph_def(graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = sess.graph.get_tensor_by_name(\"import/Placeholder:0\")\n",
    "in2 = sess.graph.get_tensor_by_name(\"import/Placeholder_1:0\")\n",
    "out = sess.graph.get_tensor_by_name(\"import/out:0\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_x_len, batch_y, batch_y_len = data_loader_train.__next__()\n",
    "batch_x, batch_x_len, batch_y, batch_y_len = data_loader_val.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get out of distribution samples\n",
    "\n",
    "import librosa\n",
    "from scipy.io import wavfile\n",
    "\n",
    "from utils.transformations import reshape4FakeCNN\n",
    "\n",
    "T = 960\n",
    "# audio_path = 'sample_audios/1545944593017.wav'\n",
    "audio_path = 'sample_audios/1545944684999.wav'\n",
    "# audio_path = 'sample_audios/1545991337572.wav'\n",
    "\n",
    "batch_x = np.zeros((1, T, 16))\n",
    "batch_x_len = np.zeros(1, dtype=np.int)\n",
    "# batch_y = np.zeros((1, 68), dtype=np.int)\n",
    "batch_y = np.ones((1, 175), dtype=np.int)*28\n",
    "# batch_y = np.zeros((1, 33), dtype=np.int)\n",
    "\n",
    "# y_text = 'hi how are you doing today i was implementing you i hope you like me'\n",
    "y_text = 'basic english is a englished based controlled language created by linguist and philosopher as an international language and as an aid for teaching english as a second language'\n",
    "# y_text = 'nice so now it works on the phone'\n",
    "batch_y[0, :len(y_text)] = text_to_int_sequence(y_text) \n",
    "\n",
    "# wavf = wavfile.read(audio_path)[1].astype(np.float)\n",
    "\n",
    "y, sr = librosa.load(audio_path, sr=44100) # librosa.util.example_audio_file()\n",
    "wavf = librosa.resample(y, sr, 16000)\n",
    "\n",
    "mfcc = librosa.feature.mfcc(\n",
    "    y=wavf,\n",
    "    sr=16000,\n",
    "    n_fft=2048,\n",
    "    hop_length=256,\n",
    "    n_mfcc=16,\n",
    "    n_mels=128,\n",
    "    fmin=0,\n",
    "    fmax=8000\n",
    ").T\n",
    "print(mfcc.shape[0])\n",
    "assert mfcc.shape[0] <= T\n",
    "\n",
    "# normalize\n",
    "mean = mfcc.mean(axis=0)\n",
    "std = mfcc.std(axis=0)\n",
    "mfcc_norm = (mfcc - mean)/std\n",
    "\n",
    "batch_x[0, :mfcc.shape[0]] = mfcc_norm\n",
    "batch_x_len[0] = mfcc.shape[0]\n",
    "\n",
    "batch_x, batch_x_len = reshape4FakeCNN(batch_x, batch_x_len, 10, 2)\n",
    "batch_x = np.reshape(batch_x, (1, batch_x.shape[1]*batch_x.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "sample_n = 0\n",
    "\n",
    "x_ = batch_x\n",
    "nb = 1\n",
    "\n",
    "print(x_.shape)\n",
    "\n",
    "result = sess.run(out, {\n",
    "    in1: x_[:nb],\n",
    "    in2: np.zeros((nb, 256))\n",
    "})\n",
    "\n",
    "result = np.reshape(result, (nb, 480, 32))[:, :, :config['num_classes']]\n",
    "sub_results_soft = softmax(result[:nb]) \n",
    "\n",
    "# prediction = model.predict(x_[sample_n].reshape(1, x_.shape[1], x_.shape[2]))\n",
    "output_length = [len(batch_y[sample_n])]\n",
    "\n",
    "pred_sample = np.reshape(sub_results_soft[sample_n], (1, result.shape[1], result.shape[2]))\n",
    "res = K.ctc_decode(pred_sample, [320])\n",
    "pred_ints = (K.eval(K.ctc_decode(\n",
    "                pred_sample, output_length)[0][0])+1).flatten().tolist()\n",
    "\n",
    "print('True transcription:\\n' + '\\n' + ''.join(int_sequence_to_text(batch_y[sample_n] + 1)))\n",
    "print('-' * 80)\n",
    "print('Predicted transcription:\\n' + '\\n' + ''.join(int_sequence_to_text(pred_ints)))\n",
    "print('-' * 80)\n",
    "print('Predicted transcription with LM:\\n' + '\\n' + wordBeamSearch(sub_results_soft[sample_n], 25, lm, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "\n",
    "char_map_str = \"' abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "char_map = {}\n",
    "index_map = {}\n",
    "index = 0\n",
    "for letter in char_map_str:\n",
    "    char_map[letter] = index\n",
    "    index_map[index + 1] = letter\n",
    "    index += 1\n",
    "\n",
    "def text_to_int_sequence(text):\n",
    "    \"\"\" Convert text to an integer sequence \"\"\"\n",
    "    return [char_map[_] for _ in text]\n",
    "\n",
    "def get_chunk_sizes(N, chunk_size):\n",
    "    full = int(N/chunk_size)\n",
    "    ch_sizes = [chunk_size for i in range(full)]\n",
    "    diff = N - full*chunk_size\n",
    "    if diff != 0:\n",
    "        ch_sizes.append(diff)\n",
    "    return ch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x)\n",
    "    return np.einsum('ijk,ij->ijk', e_x ,1/e_x.sum(axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_map_str = \"' abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "char_map = {}\n",
    "index_map = {}\n",
    "index = 0\n",
    "for letter in char_map_str:\n",
    "    char_map[letter] = index\n",
    "    index_map[index + 1] = letter\n",
    "    index += 1\n",
    "\n",
    "def int_sequence_to_text(int_sequence):\n",
    "    \"\"\" Convert an integer sequence to text \"\"\"\n",
    "    text = ''\n",
    "    for i in int_sequence:\n",
    "        if not i == 29:\n",
    "            text += index_map[i]\n",
    "    return text\n",
    "\n",
    "\n",
    "def cnn_output_length(input_length, filter_size, border_mode, stride, dilation=1):\n",
    "    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n",
    "    if border_mode == 'same':\n",
    "        output_length = input_length\n",
    "    elif border_mode == 'valid':\n",
    "        output_length = input_length - dilated_filter_size + 1\n",
    "    return (output_length + stride - 1) // stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\t\"class representing nodes in a prefix tree\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.children={} # all child elements beginning with current prefix\n",
    "\t\tself.isWord=False # does this prefix represent a word\n",
    "\t\t\n",
    "\tdef __str__(self):\n",
    "\t\ts=''\n",
    "\t\tfor (k,_) in self.children.items():\n",
    "\t\t\ts+=k\n",
    "\t\treturn 'isWord: '+str(self.isWord)+'; children: '+s\n",
    "\n",
    "\n",
    "class PrefixTree:\n",
    "\t\"prefix tree\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.root=Node()\n",
    "\n",
    "\tdef addWord(self, text):\n",
    "\t\t\"add word to prefix tree\"\n",
    "\t\tnode=self.root\n",
    "\t\tfor i in range(len(text)):\n",
    "\t\t\tc=text[i] # current char\n",
    "\t\t\tif c not in node.children:\n",
    "\t\t\t\tnode.children[c]=Node()\n",
    "\t\t\tnode=node.children[c]\n",
    "\t\t\tisLast=(i+1==len(text))\n",
    "\t\t\tif isLast:\n",
    "\t\t\t\tnode.isWord=True\n",
    "\t\t\t\t\n",
    "\tdef addWords(self, words):\n",
    "\t\tfor w in words:\n",
    "\t\t\tself.addWord(w)\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\tdef getNode(self, text):\n",
    "\t\t\"get node representing given text\"\n",
    "\t\tnode=self.root\n",
    "\t\tfor c in text:\n",
    "\t\t\tif c in node.children:\n",
    "\t\t\t\tnode=node.children[c]\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn None\n",
    "\t\treturn node\n",
    "\n",
    "\t\t\n",
    "\tdef isWord(self, text):\n",
    "\t\tnode=self.getNode(text)\n",
    "\t\tif node:\n",
    "\t\t\treturn node.isWord\n",
    "\t\treturn False\n",
    "\t\t\n",
    "\t\n",
    "\tdef getNextChars(self, text):\n",
    "\t\t\"get all characters which may directly follow given text\"\n",
    "\t\tchars=[]\n",
    "\t\tnode=self.getNode(text)\n",
    "\t\tif node:\n",
    "\t\t\tfor k,_ in node.children.items():\n",
    "\t\t\t\tchars.append(k)\n",
    "\t\treturn chars\n",
    "\t\n",
    "\t\n",
    "\tdef getNextWords(self, text):\n",
    "\t\t\"get all words of which given text is a prefix (including the text itself, it is a word)\"\n",
    "\t\twords=[]\n",
    "\t\tnode=self.getNode(text)\n",
    "\t\tif node:\n",
    "\t\t\tnodes=[node]\n",
    "\t\t\tprefixes=[text]\n",
    "\t\t\twhile len(nodes)>0:\n",
    "\t\t\t\t# put all children into list\n",
    "\t\t\t\tfor k,v in nodes[0].children.items():\n",
    "\t\t\t\t\tnodes.append(v)\n",
    "\t\t\t\t\tprefixes.append(prefixes[0]+k)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# is current node a word\n",
    "\t\t\t\tif nodes[0].isWord:\n",
    "\t\t\t\t\twords.append(prefixes[0])\n",
    "\t\t\t\t\n",
    "\t\t\t\t# remove current node\n",
    "\t\t\t\tdel nodes[0]\n",
    "\t\t\t\tdel prefixes[0]\n",
    "\t\t\t\t\n",
    "\t\treturn words\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\tdef dump(self):\n",
    "\t\tnodes=[self.root]\n",
    "\t\twhile len(nodes)>0:\n",
    "\t\t\t# put all children into list\n",
    "\t\t\tfor _,v in nodes[0].children.items():\n",
    "\t\t\t\tnodes.append(v)\n",
    "\t\t\t\n",
    "\t\t\t# dump current node\n",
    "\t\t\tprint(nodes[0])\n",
    "\t\t\t\t\n",
    "\t\t\t# remove from list\n",
    "\t\t\tdel nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class LanguageModel:\n",
    "\t\"unigram/bigram LM, add-k smoothing\"\n",
    "\tdef __init__(self, corpus, chars, wordChars):\n",
    "\t\t\"read text from filename, specify chars which are contained in dataset, specify chars which form words\"\n",
    "\t\t# read from file\n",
    "\t\tself.wordCharPattern='['+wordChars+']'\n",
    "\t\tself.wordPattern=self.wordCharPattern+'+'\n",
    "\t\twords=re.findall(self.wordPattern, corpus)\n",
    "\t\tuniqueWords=list(set(words)) # make unique\n",
    "\t\tself.numWords=len(words)\n",
    "\t\tself.numUniqueWords=len(uniqueWords)\n",
    "\t\tself.smoothing=True\n",
    "\t\tself.addK=1.0 if self.smoothing else 0.0\n",
    "\t\t\n",
    "\t\t# create unigrams\n",
    "\t\tself.unigrams={}\n",
    "\t\tfor w in words:\n",
    "\t\t\tw=w.lower()\n",
    "\t\t\tif w not in self.unigrams:\n",
    "\t\t\t\tself.unigrams[w]=0\n",
    "\t\t\tself.unigrams[w]+=1/self.numWords\n",
    "\t\t\n",
    "\t\t# create unnormalized bigrams\n",
    "\t\tbigrams={}\n",
    "\t\tfor i in range(len(words)-1):\n",
    "\t\t\tw1=words[i].lower()\n",
    "\t\t\tw2=words[i+1].lower()\n",
    "\t\t\tif w1 not in bigrams:\n",
    "\t\t\t\tbigrams[w1]={}\n",
    "\t\t\tif w2 not in bigrams[w1]:\n",
    "\t\t\t\tbigrams[w1][w2]=self.addK # add-K\n",
    "\t\t\tbigrams[w1][w2]+=1\n",
    "\t\t\t\n",
    "\t\t#normalize bigrams \n",
    "\t\tfor w1 in bigrams.keys():\n",
    "\t\t\t# sum up\n",
    "\t\t\tprobSum=self.numUniqueWords*self.addK # add-K smoothing\n",
    "\t\t\tfor w2 in bigrams[w1].keys():\n",
    "\t\t\t\tprobSum+=bigrams[w1][w2]\n",
    "\t\t\t# and divide\n",
    "\t\t\tfor w2 in bigrams[w1].keys():\n",
    "\t\t\t\tbigrams[w1][w2]/=probSum\n",
    "\t\tself.bigrams=bigrams\n",
    "\t\t\n",
    "\t\t# create prefix tree\n",
    "\t\tself.tree=PrefixTree() # create empty tree\n",
    "\t\tself.tree.addWords(words) # add all unique words to tree\n",
    "\t\t\n",
    "\t\t# list of all chars, word chars and nonword chars\n",
    "\t\tself.allChars=chars\n",
    "\t\tself.wordChars=wordChars\n",
    "\t\tself.nonWordChars=str().join(set(chars)-set(re.findall(self.wordCharPattern, chars))) # else calculate those chars\n",
    "\t\n",
    "\n",
    "\tdef getNextWords(self, text):\n",
    "\t\t\"text must be prefix of a word\"\n",
    "\t\treturn self.tree.getNextWords(text)\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef getNextChars(self, text):\n",
    "\t\t\"text must be prefix of a word\"\n",
    "\t\tnextChars=str().join(self.tree.getNextChars(text))\n",
    "\t\t\n",
    "\t\t# if in between two words or if word ends, add non-word chars\n",
    "\t\tif (text=='') or (self.isWord(text)):\n",
    "\t\t\tnextChars+=self.getNonWordChars()\n",
    "\t\t\t\n",
    "\t\treturn nextChars\n",
    "\n",
    "\t\t\n",
    "\tdef getWordChars(self):\n",
    "\t\treturn self.wordChars\n",
    "\n",
    "\t\t\n",
    "\tdef getNonWordChars(self):\n",
    "\t\treturn self.nonWordChars\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef getAllChars(self):\n",
    "\t\treturn self.allChars\n",
    "\t\n",
    "\t\n",
    "\tdef isWord(self, text):\n",
    "\t\treturn self.tree.isWord(text)\n",
    "\t\t\n",
    "\t\n",
    "\tdef getUnigramProb(self, w):\n",
    "\t\t\"prob of seeing word w.\"\n",
    "\t\tw=w.lower()\n",
    "\t\tval=self.unigrams.get(w)\n",
    "\t\tif val!=None:\n",
    "\t\t\treturn val\n",
    "\t\treturn 0\n",
    "\t\t\n",
    "\t\n",
    "\tdef getBigramProb(self, w1, w2):\n",
    "\t\t\"prob of seeing words w1 w2 next to each other.\"\n",
    "\t\tw1=w1.lower()\n",
    "\t\tw2=w2.lower()\n",
    "\t\tval1=self.bigrams.get(w1)\n",
    "\t\tif val1!=None:\n",
    "\t\t\tval2=val1.get(w2)\n",
    "\t\t\tif val2!=None:\n",
    "\t\t\t\treturn val2\n",
    "\t\t\treturn self.addK/(self.getUnigramProb(w1)*self.numUniqueWords+self.numUniqueWords)\n",
    "\t\treturn 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class Optical:\n",
    "\t\"optical score of beam\"\n",
    "\tdef __init__(self, prBlank=0, prNonBlank=0):\n",
    "\t\tself.prBlank=prBlank # prob of ending with a blank\n",
    "\t\tself.prNonBlank=prNonBlank # prob of ending with a non-blank\n",
    "\n",
    "\n",
    "class Textual:\n",
    "\t\"textual score of beam\"\n",
    "\tdef __init__(self, text=''):\n",
    "\t\tself.text=text\n",
    "\t\tself.wordHist=[] # history of words so far\n",
    "\t\tself.wordDev='' # developing word\n",
    "\t\tself.prUnnormalized=1.0\n",
    "\t\tself.prTotal=1.0\n",
    "\n",
    "\n",
    "class Beam:\n",
    "\t\"beam with text, optical and textual score\"\n",
    "\tdef __init__(self, lm, useNGrams):\n",
    "\t\t\"creates genesis beam\"\n",
    "\t\tself.optical=Optical(1.0, 0.0)\n",
    "\t\tself.textual=Textual('')\n",
    "\t\tself.lm=lm\n",
    "\t\tself.useNGrams=useNGrams\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef mergeBeam(self, beam):\n",
    "\t\t\"merge probabilities of two beams with same text\"\n",
    "\t\t\n",
    "\t\tif self.getText()!=beam.getText():\n",
    "\t\t\traise Exception('mergeBeam: texts differ')\n",
    "\t\t\n",
    "\t\tself.optical.prNonBlank+=beam.getPrNonBlank()\n",
    "\t\tself.optical.prBlank+=beam.getPrBlank()\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef getText(self):\n",
    "\t\treturn self.textual.text\n",
    "\t\t\n",
    "\t\n",
    "\tdef getPrBlank(self):\n",
    "\t\treturn self.optical.prBlank\n",
    "\t\n",
    "\t\n",
    "\tdef getPrNonBlank(self):\n",
    "\t\treturn self.optical.prNonBlank\n",
    "\t\n",
    "\n",
    "\tdef getPrTotal(self):\n",
    "\t\treturn self.getPrBlank()+self.getPrNonBlank()\n",
    "\t\n",
    "\t\n",
    "\tdef getPrTextual(self):\n",
    "\t\treturn self.textual.prTotal\n",
    "\t\n",
    "\t\n",
    "\tdef getNextChars(self):\n",
    "\t\treturn self.lm.getNextChars(self.textual.wordDev)\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef createChildBeam(self, newChar, prBlank, prNonBlank):\n",
    "\t\t\"extend beam by new character and set optical score\"\n",
    "\t\tbeam=Beam(self.lm, self.useNGrams)\n",
    "\t\t\n",
    "\t\t# copy textual information\n",
    "\t\tbeam.textual=copy.deepcopy(self.textual)\n",
    "\t\tbeam.textual.text+=newChar\n",
    "\t\t\n",
    "\t\t# do textual calculations only if beam gets extended\n",
    "\t\tif newChar!='':\n",
    "\t\t\tif self.useNGrams: # use unigrams and bigrams \n",
    "\t\t\t\n",
    "\t\t\t\t# if new char occurs inside a word\n",
    "\t\t\t\tif newChar in beam.lm.getWordChars():\n",
    "\t\t\t\t\tbeam.textual.wordDev+=newChar\n",
    "\t\t\t\t\tnextWords=beam.lm.getNextWords(beam.textual.wordDev)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# no complete word in text, then use unigram of all possible next words\n",
    "\t\t\t\t\tnumWords=len(beam.textual.wordHist)\n",
    "\t\t\t\t\tprSum=0\n",
    "\t\t\t\t\tif numWords==0:\n",
    "\t\t\t\t\t\tfor w in nextWords:\n",
    "\t\t\t\t\t\t\tprSum+=beam.lm.getUnigramProb(w)\n",
    "\t\t\t\t\t# take last complete word and sum up bigrams of all possible next words\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tlastWord=beam.textual.wordHist[-1]\n",
    "\t\t\t\t\t\tfor w in nextWords:\n",
    "\t\t\t\t\t\t\tprSum+=beam.lm.getBigramProb(lastWord, w)\n",
    "\t\t\t\t\tbeam.textual.prTotal=beam.textual.prUnnormalized*prSum\n",
    "\t\t\t\t\tbeam.textual.prTotal=beam.textual.prTotal**(1/(numWords+1)) if numWords>=1 else beam.textual.prTotal\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t# if new char does not occur inside a word\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# if current word is not empty, add it to history\n",
    "\t\t\t\t\tif beam.textual.wordDev!='':\n",
    "\t\t\t\t\t\tbeam.textual.wordHist.append(beam.textual.wordDev)\n",
    "\t\t\t\t\t\tbeam.textual.wordDev=''\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# score with unigram (first word) or bigram (all other words) probability\n",
    "\t\t\t\t\t\tnumWords=len(beam.textual.wordHist)\n",
    "\t\t\t\t\t\tif numWords==1:\n",
    "\t\t\t\t\t\t\tbeam.textual.prUnnormalized*=beam.lm.getUnigramProb(beam.textual.wordHist[-1])\n",
    "\t\t\t\t\t\t\tbeam.textual.prTotal=beam.textual.prUnnormalized\n",
    "\t\t\t\t\t\telif numWords>=2:\n",
    "\t\t\t\t\t\t\tbeam.textual.prUnnormalized*=beam.lm.getBigramProb(beam.textual.wordHist[-2], beam.textual.wordHist[-1])\n",
    "\t\t\t\t\t\t\tbeam.textual.prTotal=beam.textual.prUnnormalized**(1/numWords)\n",
    "\t\t\t\n",
    "\t\t\telse: # don't use unigrams and bigrams, just keep wordDev up to date\n",
    "\t\t\t\tif newChar in beam.lm.getWordChars():\n",
    "\t\t\t\t\tbeam.textual.wordDev+=newChar\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbeam.textual.wordDev=''\n",
    "\t\t\n",
    "\t\t# set optical information\n",
    "\t\tbeam.optical.prBlank=prBlank\n",
    "\t\tbeam.optical.prNonBlank=prNonBlank\n",
    "\t\treturn beam\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef __str__(self):\n",
    "\t\treturn '\"'+self.getText()+'\"'+';'+str(self.getPrTotal())+';'+str(self.getPrTextual())+';'+str(self.textual.prUnnormalized)\n",
    "\n",
    "\n",
    "class BeamList:\n",
    "\t\"list of beams at specific time-step\"\n",
    "\tdef __init__(self):\n",
    "\t\tself.beams={}\n",
    "\t\t\n",
    "\n",
    "\tdef addBeam(self, beam):\n",
    "\t\t\"add or merge new beam into list\"\n",
    "\t\t# add if text not yet known\n",
    "\t\tif beam.getText() not in self.beams:\n",
    "\t\t\tself.beams[beam.getText()]=beam\n",
    "\t\t# otherwise merge with existing beam\n",
    "\t\telse:\n",
    "\t\t\tself.beams[beam.getText()].mergeBeam(beam)\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef getBestBeams(self, num):\n",
    "\t\t\"return best beams, specify the max. number of beams to be returned (beam width)\"\n",
    "\t\tu=[v for (_,v) in self.beams.items()]\n",
    "\t\tlmWeight=1\n",
    "\t\treturn sorted(u, reverse=True, key=lambda x:x.getPrTotal()*(x.getPrTextual()**lmWeight))[:num]\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef deletePartialBeams(self, lm):\n",
    "\t\t\"delete beams for which last word is not finished\"\n",
    "\t\tfor (k,v) in self.beams.items():\n",
    "\t\t\tlastWord=v.textual.wordDev\n",
    "\t\t\tif (lastWord!='') and (not lm.isWord(lastWord)):\n",
    "\t\t\t\tdel self.beams[k]\n",
    "\t\n",
    "\t\n",
    "\tdef completeBeams(self, lm):\n",
    "\t\t\"complete beams such that last word is complete word\"\n",
    "\t\tfor (_,v) in self.beams.items():\n",
    "\t\t\tlastPrefix=v.textual.wordDev\n",
    "\t\t\tif lastPrefix=='' or lm.isWord(lastPrefix):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\t\t# get word candidates for this prefix\n",
    "\t\t\twords=lm.getNextWords(lastPrefix)\n",
    "\t\t\t# if there is just one candidate, then the last prefix can be extended to \n",
    "\t\t\tif len(words)==1:\n",
    "\t\t\t\tword=words[0]\n",
    "\t\t\t\tv.textual.text+=word[len(lastPrefix)-len(word):]\n",
    "\n",
    "\n",
    "\tdef dump(self):\n",
    "\t\tfor k in self.beams.keys():\n",
    "\t\t\tprint(unicode(self.beams[k]).encode('ascii', 'replace')) # map to ascii if possible (for py2 and windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordBeamSearch(mat, beamWidth, lm, useNGrams):\n",
    "\t\"decode matrix, use given beam width and language model\"\n",
    "\tchars=lm.getAllChars()\n",
    "\tblankIdx=len(chars) # blank label is supposed to be last label in RNN output\n",
    "\tmaxT,_=mat.shape # shape of RNN output: TxC\n",
    "\t\n",
    "\tgenesisBeam=Beam(lm, useNGrams) # empty string\n",
    "\tlast=BeamList() # list of beams at time-step before beginning of RNN output\n",
    "\tlast.addBeam(genesisBeam) # start with genesis beam\n",
    "\t\n",
    "\t# go over all time-steps\n",
    "\tfor t in range(maxT):\n",
    "\t\tcurr=BeamList() # list of beams at current time-step\n",
    "\t\t\n",
    "\t\t# go over best beams\n",
    "\t\tbestBeams=last.getBestBeams(beamWidth) # get best beams\n",
    "\t\tfor beam in bestBeams:\n",
    "\t\t\t# calc probability that beam ends with non-blank\n",
    "\t\t\tprNonBlank=0\n",
    "\t\t\tif beam.getText()!='':\n",
    "\t\t\t\t# char at time-step t must also occur at t-1\n",
    "\t\t\t\tlabelIdx=chars.index(beam.getText()[-1])\n",
    "\t\t\t\tprNonBlank=beam.getPrNonBlank()*mat[t, labelIdx]\n",
    "\t\t\t\n",
    "\t\t\t# calc probability that beam ends with blank\n",
    "\t\t\tprBlank=beam.getPrTotal()*mat[t, blankIdx]\n",
    "\t\t\t\n",
    "\t\t\t# save result\n",
    "\t\t\tcurr.addBeam(beam.createChildBeam('', prBlank, prNonBlank))\n",
    "\t\t\t\n",
    "\t\t\t# extend current beam with characters according to language model\n",
    "\t\t\tnextChars=beam.getNextChars()\n",
    "\t\t\tfor c in nextChars:\n",
    "\t\t\t\t# extend current beam with new character\n",
    "\t\t\t\tlabelIdx=chars.index(c)\n",
    "\t\t\t\tif beam.getText()!='' and beam.getText()[-1]==c: \n",
    "\t\t\t\t\tprNonBlank=mat[t, labelIdx]*beam.getPrBlank() # same chars must be separated by blank\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprNonBlank=mat[t, labelIdx]*beam.getPrTotal() # different chars can be neighbours\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t# save result\n",
    "\t\t\t\tcurr.addBeam(beam.createChildBeam(c, 0, prNonBlank))\n",
    "\t\t\n",
    "\t\t# move current beams to next time-step\n",
    "\t\tlast=curr\n",
    "\t\t\n",
    "\t# return most probable beam\n",
    "\tlast.completeBeams(lm)\n",
    "\tbestBeams=last.getBestBeams(1) # sort by probability\n",
    "\treturn bestBeams[0].getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = open('word_list/google-10000-english.txt', 'r').read()\n",
    "# words = set(corpus.split('\\n'))\n",
    "\n",
    "# # Some primitive pre-processing\n",
    "# for word in \"bcdefghjklmnopqrstuvwxyz\":\n",
    "#     words.remove(word)\n",
    "    \n",
    "# for word in ['es', 'ti', 'te', 'wi', 'fr', 'rc', 'hr', 'ict', 'nat', 'ng', 'ld', 'en', 'ix', 'xp', 'don', 'wu', 'min', 'oe', 'hs', 'ol', 'lo', 'le']:\n",
    "#     words.remove(word)\n",
    "\n",
    "# words.add('practicable')\n",
    "    \n",
    "# lm = LanguageModel(\" \".join(words), \"' abcdefghijklmnopqrstuvwxyz\", \"abcdefghijklmnopqrstuvwxyz\")\n",
    "\n",
    "corpus = open('word_list/cleaned_words_10k_nofreq.txt', 'r').read()\n",
    "\n",
    "lm = LanguageModel(corpus, \"' abcdefghijklmnopqrstuvwxyz\", \"abcdefghijklmnopqrstuvwxyz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
